# main.py
import os
from langchain_ollama import OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.prompts import PromptTemplate
import Analysepdf as Apdf
import Analysedocs as Adoc

# ===============================
# æ–‡ä»¶åŠ è½½å‡½æ•°
# ===============================
def load_file(file_path, file_type):
    """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½ä¸åŒæ–‡æ¡£"""
    if file_type == 'pdf':
        return Apdf.load_pdf(file_path)
    elif file_type == 'docx':
        return Adoc.load_docx(file_path)
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

# ===============================
# åˆå§‹åŒ–æœ¬åœ° Ollama LLM
# ===============================
def init_llm(model_name="deepseek-r1:8b"):
    """åˆå§‹åŒ–æœ¬åœ° Ollama æ¨¡å‹"""
    return OllamaLLM(
        model=model_name,
        temperature=0
    )
# ===============================
# RAG ä¸»æµç¨‹
# ===============================
def rag_pipeline(documents_dir, user_query, model_name="deepseek-r1:8b"):
    """RAG æµç¨‹ï¼šåŠ è½½æ–‡æ¡£ â†’ å‘é‡ç´¢å¼• â†’ é—®ç­”"""
    # 1. åŠ è½½æ–‡æ¡£
    all_texts = []
    for file_name in os.listdir(documents_dir):
        file_path = os.path.join(documents_dir, file_name)
        if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.txt')):
            ext = file_name.split('.')[-1].lower()
            text = load_file(file_path, ext)
            all_texts.append({'content': text, 'filename': file_name})

    if not all_texts:
        return "âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•è·¯å¾„"

    print("âœ… æ–‡æ¡£å·²åŠ è½½ï¼Œåˆ›å»ºå‘é‡ç´¢å¼•...")

    # 2. åµŒå…¥æ¨¡å‹ï¼ˆè¿™é‡Œç”¨ HuggingFace sentence-transformerï¼Œæœ¬åœ°å¯è·‘ï¼‰
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
    metadatas = [{"source": doc['filename']} for doc in all_texts]
    db = FAISS.from_texts(
        [doc['content'] for doc in all_texts],
        embeddings,
        metadatas=metadatas
    )
    db.save_local("faiss_index")
    print("âœ… å‘é‡æ•°æ®åº“å·²åˆ›å»º")

    # 4. åˆå§‹åŒ–æœ¬åœ° LLM
    llm = init_llm(model_name=model_name)

    # 5. åˆ›å»ºæ£€ç´¢å™¨
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5}
    )

    # 6. åˆ›å»ºé—®ç­”é“¾ï¼ˆæ–°ç‰ˆå†™æ³•ï¼‰
    prompt_template = "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜: {question}\nç­”æ¡ˆ:"
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… é—®ç­”é“¾å·²å‡†å¤‡å¥½ï¼Œæ­£åœ¨å›ç­”é—®é¢˜...")

    try:
        response = qa_chain.invoke({"query": user_query})

        # æå–ç­”æ¡ˆå’Œæ¥æº
        answer = response["result"]
        source_docs = response.get("source_documents", [])

        sources = []
        for doc in source_docs:
            metadata = doc.metadata
            page_content = doc.page_content
            if isinstance(metadata, dict) and "source" in metadata:
                snippet = page_content[:50] + "..." if len(page_content) > 50 else page_content
                sources.append({"filename": metadata["source"], "text": snippet})

        return {"answer": answer, "sources": sources}

    except Exception as e:
        print(f"âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"


# ===============================
# ä¸»ç¨‹åºå…¥å£
# ===============================
if __name__ == "__main__":
    documents_dir = "./my_documents"
    query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜: ")

    # é»˜è®¤ä½¿ç”¨æœ¬åœ° deepseek-r1:8b æ¨¡å‹
    result = rag_pipeline(documents_dir, query, model_name="deepseek-r1:8b")

    if isinstance(result, dict):
        print("\nğŸ“Œ ç­”æ¡ˆ:")
        print(result["answer"])

        print("\nğŸ“Œ ç›¸å…³å†…å®¹æ¥æºï¼ˆç‰‡æ®µï¼‰:")
        for src in result["sources"]:
            print(f"- æ¥æº: {src['filename']}")
            print(f"  å†…å®¹: {src['text']}\n")
    else:
        print(result)
