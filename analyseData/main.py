# main.py
import os
from langchain_ollama import OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.prompts import PromptTemplate
import Analysepdf as Apdf
import Analysedocs as Adoc
rom langchain.vectorstores import FAISS
import os
import pickle


# ===============================
# æ–‡ä»¶åŠ è½½å‡½æ•°
# ===============================
def load_file(file_path, file_type):
    """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½ä¸åŒæ–‡æ¡£"""
    if file_type == 'pdf':
        return Apdf.load_pdf(file_path)
    elif file_type == 'docx':
        return Adoc.load_docx(file_path)
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()


def load_or_create_vector_db(documents_dir, index_path="analyseData/faiss", embedding_model_path=r"C:\Users\325043\Desktop\model\all-MiniLM-L6-v2"):
    """åŠ è½½æˆ–åˆ›å»º FAISS å‘é‡æ•°æ®åº“"""
    
    # å¦‚æœå­˜åœ¨å·²ä¿å­˜çš„ç´¢å¼•ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(index_path):
        print("âœ… åŠ è½½å·²ä¿å­˜çš„å‘é‡æ•°æ®åº“...")
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)
        return FAISS.load_local(index_path, embeddings)
    
    # å¦åˆ™ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“
    print("ğŸ”„ æœªæ‰¾åˆ°å‘é‡æ•°æ®åº“ï¼Œå¼€å§‹é‡æ–°æ„å»º...")
    all_texts = []
    for file_name in os.listdir(documents_dir):
        file_path = os.path.join(documents_dir, file_name)
        if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.txt')):
            ext = file_name.split('.')[-1].lower()
            text = load_file(file_path, ext)
            all_texts.append({'content': text, 'filename': file_name})

    if not all_texts:
        raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•è·¯å¾„")

    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)
    metadatas = [{"source": doc['filename']} for doc in all_texts]

    db = FAISS.from_texts([doc['content'] for doc in all_texts], embeddings, metadatas=metadatas)
    db.save_local(index_path)
    print("âœ… å‘é‡æ•°æ®åº“å·²åˆ›å»ºå¹¶ä¿å­˜")

    return db

# ===============================
# åˆå§‹åŒ–æœ¬åœ° Hugging Face LLM
# ===============================
def init_llm(model_name_or_path=r"C:\Users\325043\Desktop\model\DeepSeek-R1-Distill-Qwen-1.5B"):
    """
    åˆå§‹åŒ–æœ¬åœ° Hugging Face æ¨¡å‹
    model_name_or_path: Hugging Face Hub åç§°æˆ–æœ¬åœ°è·¯å¾„
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"

    #tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True, cache_dir='/home/{username}/huggingface')
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None
)


    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if device == "cuda" else -1,
        max_new_tokens=512,
        do_sample=False
    )

    # ç”¨ langchain çš„ HuggingFacePipeline å°è£…
    return HuggingFacePipeline(pipeline=pipe)

def rag_pipeline(documents_dir, user_query, model_name=r"C:\Users\325043\Desktop\model\DeepSeek-R1-Distill-Qwen-1.5B"):
    """RAG æµç¨‹ï¼šåŠ è½½æˆ–ä½¿ç”¨å·²ä¿å­˜çš„å‘é‡ç´¢å¼• â†’ é—®ç­”"""
    
    # 1. åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“
    db = load_or_create_vector_db(documents_dir)

    # 2. åˆå§‹åŒ–æœ¬åœ° Hugging Face æ¨¡å‹
    llm = init_llm(model_name_or_path=model_name)

    # 3. åˆ›å»ºæ£€ç´¢å™¨
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5}
    )

    # 4. åˆ›å»ºé—®ç­”é“¾
    prompt_template = "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜: {question}\nç­”æ¡ˆ:"
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… é—®ç­”é“¾å·²å‡†å¤‡å¥½ï¼Œæ­£åœ¨å›ç­”é—®é¢˜...")

    try:
        response = qa_chain.invoke({"query": user_query})
        answer = response["result"]
        source_docs = response.get("source_documents", [])

        sources = []
        for doc in source_docs:
            metadata = doc.metadata
            page_content = doc.page_content
            if isinstance(metadata, dict) and "source" in metadata:
                snippet = page_content[:50] + "..." if len(page_content) > 50 else page_content
                sources.append({"filename": metadata["source"], "text": snippet})

        return {"answer": answer, "sources": sources}

    except Exception as e:
        print(f"âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"



# ===============================
# ä¸»ç¨‹åºå…¥å£
# ===============================
if __name__ == "__main__":
    documents_dir = "analyseData/lib"
    #documents_dir = r""
    query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜: ")

    # ä½¿ç”¨ Hugging Face æœ¬åœ°æ¨¡å‹
    result = rag_pipeline(documents_dir, query, model_name="analyseData/modle/DeepSeek-R1-Distill-Qwen-1.5B")

    if isinstance(result, dict):
        print("\nğŸ“Œ ç­”æ¡ˆ:")
        print(result["answer"])

        print("\nğŸ“Œ ç›¸å…³å†…å®¹æ¥æºï¼ˆç‰‡æ®µï¼‰:")
        for src in result["sources"]:
            print(f"- æ¥æº: {src['filename']}")
            print(f"  å†…å®¹: {src['text']}\n")
    else:
        print(result)
