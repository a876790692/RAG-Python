{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e8ac89",
   "metadata": {},
   "source": [
    "\n",
    "# 📘 Ollama 使用说明及无管理员权限时的应对方案\n",
    "\n",
    "Ollama 是一个方便的本地大模型推理工具，但它的安装通常需要管理员权限（特别是在 Windows 和 macOS 下）。  \n",
    "如果没有管理员权限，直接安装 Ollama 可能会遇到权限不足的问题。\n",
    "\n",
    "## 🔹 没有管理员权限时的应对方式\n",
    "\n",
    "1. **使用便携版 Ollama**\n",
    "   - 从 GitHub Releases 页面下载可执行文件，放在用户目录下运行，不写入系统目录。\n",
    "   - 启动后同样可以通过 `http://localhost:11434` 调用 API。\n",
    "\n",
    "2. **使用 llama.cpp / llama-cpp-python 替代**\n",
    "   - 在用户目录下安装：  \n",
    "     ```bash\n",
    "     pip install --user llama-cpp-python\n",
    "     ```  \n",
    "   - 不需要管理员权限，即可本地加载 `.gguf` 格式模型运行。\n",
    "\n",
    "3. **远程调用 Ollama 服务**\n",
    "   - 在有权限的服务器或另一台电脑上安装 Ollama。\n",
    "   - 在本机程序中通过 HTTP 请求调用远程的 Ollama 接口，例如：  \n",
    "     ```python\n",
    "     import requests\n",
    "\n",
    "     res = requests.post(\"http://server-ip:11434/api/generate\", json={\n",
    "         \"model\": \"mistral\",\n",
    "         \"prompt\": \"Hello!\"\n",
    "     })\n",
    "     print(res.json())\n",
    "     ```\n",
    "\n",
    "4. **使用云端 API 替代**\n",
    "   - 如果只需要调用模型，可以使用 Hugging Face Hub 或 OpenAI API。\n",
    "   - 不依赖本地安装，避免权限问题。\n",
    "\n",
    "## ✅ 推荐做法\n",
    "- **优先选择 llama-cpp-python**，适合没有管理员权限的本地环境。\n",
    "- 如果硬件不够或依赖安装受限，可以选择 **远程调用** 或 **云 API**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e977518caa323b",
   "metadata": {},
   "source": [
    "RAG 系统入门指南 - Python 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56deb8c1c1f9d15a",
   "metadata": {},
   "source": [
    "所需库和工具清单"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install --user llama-cpp-python",
   "id": "8c4e5e4bbf0bfef2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2bb2d6a2f92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0631da382368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e15d70888087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c61be6f9965cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0edaf5a015efa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:55:12.928536Z",
     "start_time": "2025-09-16T09:55:09.013184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement lanngchain_ollama (from versions: none)\n",
      "ERROR: No matching distribution found for lanngchain_ollama\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc691e462cb8a7a8",
   "metadata": {},
   "source": [
    "系统架构解析\n",
    "RAG系统主要包含三个组件：\n",
    "\n",
    "检索器：从知识库中找到与用户查询最相关的文本片段\n",
    "嵌入引擎：将文档内容转化为向量以便高效检索\n",
    "生成器：基于检索到的内容创建最终回答\n",
    "下面是完整的代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a051b6c06ee5de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:56:22.518592Z",
     "start_time": "2025-09-16T09:56:16.169301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\81803\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from PyPDF2 import PdfReader  # 处理PDF文件\n",
    "from docx import Document  # 处理Word文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69c0b1620f0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docx(file_path):\n",
    "    \"\"\"加载Word文档内容\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad6934e83d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    \"\"\"加载PDF文件内容\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_reader = PdfReader(f)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() or \"\"  # 处理可能的None值\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a682e38a37c39a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:56:30.207511Z",
     "start_time": "2025-09-16T09:56:30.194020Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 文件加载函数\n",
    "# ===============================\n",
    "def load_file(file_path, file_type):\n",
    "    \"\"\"根据文件类型加载不同文档\"\"\"\n",
    "    if file_type == 'pdf':\n",
    "        return load_pdf(file_path)\n",
    "    elif file_type == 'docx':\n",
    "        return load_docx(file_path)\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "# ===============================\n",
    "# 初始化本地 Ollama LLM\n",
    "# ===============================\n",
    "def init_llm(model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"初始化本地 Ollama 模型\"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=model_name,\n",
    "        temperature=0\n",
    "    )\n",
    "# ===============================\n",
    "# RAG 主流程\n",
    "# ===============================\n",
    "def rag_pipeline(documents_dir, user_query, model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"RAG 流程：加载文档 → 向量索引 → 问答\"\"\"\n",
    "    # 1. 加载文档\n",
    "    all_texts = []\n",
    "    for file_name in os.listdir(documents_dir):\n",
    "        file_path = os.path.join(documents_dir, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.txt')):\n",
    "            ext = file_name.split('.')[-1].lower()\n",
    "            text = load_file(file_path, ext)\n",
    "            all_texts.append({'content': text, 'filename': file_name})\n",
    "\n",
    "    if not all_texts:\n",
    "        return \"❌ 没有找到任何文档，请检查目录路径\"\n",
    "\n",
    "    print(\"✅ 文档已加载，创建向量索引...\")\n",
    "\n",
    "    # 2. 嵌入模型（这里用 HuggingFace sentence-transformer，本地可跑）\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 3. 创建向量数据库\n",
    "    metadatas = [{\"source\": doc['filename']} for doc in all_texts]\n",
    "    db = FAISS.from_texts(\n",
    "        [doc['content'] for doc in all_texts],\n",
    "        embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    db.save_local(\"faiss_index\")\n",
    "    print(\"✅ 向量数据库已创建\")\n",
    "\n",
    "    # 4. 初始化本地 LLM\n",
    "    llm = init_llm(model_name=model_name)\n",
    "\n",
    "    # 5. 创建检索器\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # 6. 创建问答链（新版写法）\n",
    "    prompt_template = \"根据以下文档回答问题：\\n{context}\\n\\n问题: {question}\\n答案:\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ 问答链已准备好，正在回答问题...\")\n",
    "\n",
    "    try:\n",
    "        response = qa_chain.invoke({\"query\": user_query})\n",
    "\n",
    "        # 提取答案和来源\n",
    "        answer = response[\"result\"]\n",
    "        source_docs = response.get(\"source_documents\", [])\n",
    "\n",
    "        sources = []\n",
    "        for doc in source_docs:\n",
    "            metadata = doc.metadata\n",
    "            page_content = doc.page_content\n",
    "            if isinstance(metadata, dict) and \"source\" in metadata:\n",
    "                snippet = page_content[:50] + \"...\" if len(page_content) > 50 else page_content\n",
    "                sources.append({\"filename\": metadata[\"source\"], \"text\": snippet})\n",
    "\n",
    "        return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理查询时出错: {e}\")\n",
    "        return \"抱歉，我无法回答这个问题，请提供更多上下文信息。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f3b2f4a74aada2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T09:59:43.947565Z",
     "start_time": "2025-09-16T09:56:34.507306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 文档已加载，创建向量索引...\n",
      "✅ 向量数据库已创建\n",
      "✅ 问答链已准备好，正在回答问题...\n",
      "\n",
      "📌 答案:\n",
      "<think>\n",
      "嗯，用户分享了一个关于使用LlamaIndex构建中医临床诊疗智能助手的PDF文档，并询问从中学到什么。首先需要理解这个文档的内容和结构。\n",
      "\n",
      "好的，我看到了很多代码片段，看起来是关于如何使用LlamaIndex结合本地大模型（如Qwen-7B）来实现检索增强生成(RAG)系统的教程或笔记。这些内容涵盖了多个方面：\n",
      "\n",
      "1. **基础RAG流程**：介绍了如何构建一个基于文档的问答系统。\n",
      "2. **本地部署模型**：展示了如何在LlamaIndex中使用HuggingFaceLLM和HuggingFaceEmbedding来加载本地的大语言模型（如Qwen）和嵌入模型（如bge-base-zh-v1___5）。\n",
      "3. **自定义Prompt模板**：提供了如何自定义英文prompt template的方法，并展示了如何用中文编写这些模板。\n",
      "\n",
      "用户还提到了一个查询示例：\"不耐疲劳，口燥、咽干可能是哪些证候？\"，并展示了一个完整的RAG流程。这包括检索相关文档片段，然后根据这些片段生成回答。\n",
      "\n",
      "现在，我需要思考一下从这个PDF中能学到什么内容：\n",
      "\n",
      "- **如何使用LlamaIndex进行RAG**：了解了如何构建一个基于检索增强的问答系统。\n",
      "- **如何自定义Prompt模板**：学会了如何在LlamaIndex中使用中文prompt template，并更新到查询引擎中。\n",
      "- **模型和嵌入模型的选择与配置**：知道如何选择并加载本地的大语言模型（LLM）和嵌入模型，以及它们在RAG中的作用。\n",
      "\n",
      "用户的问题是：“中医临床诊疗智能助手.pdf中能学到什么”，我需要根据这些信息来回答。首先，我可以思考一下这个PDF文档可能包含的内容：\n",
      "\n",
      "1. **基础概念**：介绍什么是RAG、LlamaIndex等。\n",
      "2. **技术实现细节**：如何使用LlamaDebugHandler进行调试和事件回溯。\n",
      "3. **自定义Prompt的方法**：在中文环境下定制prompt template。\n",
      "\n",
      "用户提供的示例查询是关于中医证候的，所以这个助手可能专注于医疗领域的应用。我可以基于这些信息来回答用户的问题。\n",
      "</think>\n",
      "根据你提供的文档内容，我们可以学到以下几点：\n",
      "\n",
      "1. **RAG（检索增强生成）技术的应用**\n",
      "   - 了解了如何使用LlamaIndex结合本地大模型实现检索增强生成系统。\n",
      "\n",
      "2. **自定义Prompt模板**\n",
      "   - 学会了在LlamaIndex中自定义中文的prompt template，并将其应用到查询引擎中，以提高回答质量。\n",
      "   - 示例展示了如何通过修改system prompt和QA提示来定制模型的回答风格和内容严谨性。\n",
      "\n",
      "3. **调试与事件回溯**\n",
      "   - 使用`LlamaDebugHandler`可以追踪LLM调用过程中的输入输出信息，帮助理解RAG流程的底层实现细节。\n",
      "\n",
      "4. **本地大模型部署**\n",
      "   - 学会了如何使用HuggingFace上的开源模型（如Qwen-7B）作为本地大语言模型，并配置相关参数以优化性能。\n",
      "   - 了解了如何加载和更新嵌入模型，以及如何管理回调机制来调试系统。\n",
      "\n",
      "5. **文档追踪与检索**\n",
      "   - 学会了在查询时获取被检索到的上下文片段（top-k相似度最高的chunk），以便理解模型是如何基于这些信息生成回答的。\n",
      "   - 了解了RAG过程中，模型通过检索相关文本并结合这些文本进行回复的具体流程。\n",
      "\n",
      "6. **严谨性与医疗领域应用**\n",
      "   - 学会了如何在prompt中强调助手的角色和回答要求（如“尽可能严谨”），以提升在医疗领域的专业性和可靠性。\n",
      "\n",
      "7. **代码实现细节**\n",
      "   - 掌握了如何配置LlamaIndex的查询引擎，包括设置回调管理器、更新提示模板等。\n",
      "   - 了解了如何使用`CallbackManager`来调试和跟踪模型调用过程中的事件对（输入输出）。\n",
      "\n",
      "8. **中文prompt template的支持**\n",
      "   - 学会了在LlamaIndex中使用中文prompt template，并将其应用到查询引擎的合成器中，以提升生成回答的质量。\n",
      "\n",
      "9. **系统集成与优化**\n",
      "   - 通过示例代码展示了如何将大模型和嵌入模型整合到一个完整的RAG系统中，并进行调试和优化。\n",
      "\n",
      "总的来说，这个文档提供了构建医疗领域智能助手的具体方法、技巧以及实现细节，特别是在中文prompt template的使用上。\n",
      "\n",
      "📌 相关内容来源（片段）:\n",
      "- 来源: 新建 Microsoft Word 文档.docx\n",
      "  内容: 我是智能助手\n",
      "\n",
      "- 来源: 中医临床诊疗智能助手.pdf\n",
      "  内容: 中医临床诊疗智能助手 - RAG 项目实战\n",
      "💡 这节课会带给你\n",
      "1. 如何用你的垂域数据补充 LLM...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 主程序入口\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    documents_dir = \"./my_documents\"\n",
    "    query = input(\"请输入你的问题: \")\n",
    "\n",
    "    # 默认使用本地 deepseek-r1:8b 模型\n",
    "    result = rag_pipeline(documents_dir, query, model_name=\"deepseek-r1:8b\")\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        print(\"\\n📌 答案:\")\n",
    "        print(result[\"answer\"])\n",
    "\n",
    "        print(\"\\n📌 相关内容来源（片段）:\")\n",
    "        for src in result[\"sources\"]:\n",
    "            print(f\"- 来源: {src['filename']}\")\n",
    "            print(f\"  内容: {src['text']}\\n\")\n",
    "    else:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee96a9c7df423c1",
   "metadata": {},
   "source": [
    "准备文档目录："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e0e73f800da07",
   "metadata": {},
   "source": [
    "创建一个名为my_documents的文件夹（或修改代码中的路径）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b717c62dbc237",
   "metadata": {},
   "source": [
    "将你的PDF、Word和TXT文档放入该文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465c33d7dfa35ff",
   "metadata": {},
   "source": [
    "运行程序："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7e6ebe22497f0",
   "metadata": {},
   "source": [
    "技术解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92037f7b6b83206",
   "metadata": {},
   "source": [
    "核心组件说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e48d05f7c26ce1",
   "metadata": {},
   "source": [
    "文档加载器：使用PyPDF2和python-docx库处理不同格式的文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fee90faed1eb5b",
   "metadata": {},
   "source": [
    "嵌入模型：采用HuggingFace上的all-MiniLM-L6-v2模型（轻量级、开源）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa5914396a943",
   "metadata": {},
   "source": [
    "向量数据库：FAISS提供高效的相似度搜索功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466740bf9c5beb53",
   "metadata": {},
   "source": [
    "问答链：使用RetrievalQA结合检索和生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986903eab3e46571",
   "metadata": {},
   "source": [
    "关键步骤分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980e03724eb5b8",
   "metadata": {},
   "source": [
    "文档加载与预处理："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bde9bd60f347b0",
   "metadata": {},
   "source": [
    "读取PDF文件内容（每页提取文本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a522070222f5ee",
   "metadata": {},
   "source": [
    "解析Word文档内容（合并所有段落）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81aab120ee96e28",
   "metadata": {},
   "source": [
    "向量化过程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2f1b2eea3b8b6",
   "metadata": {},
   "source": [
    "将文档内容转换为数字向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa3c76b09c8fd7",
   "metadata": {},
   "source": [
    "使用FAISS建立索引，便于快速相似度搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434eb0d81764e0e",
   "metadata": {},
   "source": [
    "查询处理流程："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8bd249ff0e052",
   "metadata": {},
   "source": [
    "用户输入问题 → 检索相关文档片段 → 生成回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d5cd93856eb66",
   "metadata": {},
   "source": [
    "回答包含原始上下文信息，提高准确性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3eb7dc206294f2",
   "metadata": {},
   "source": [
    "扩展建议"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6b2c27448e59f",
   "metadata": {},
   "source": [
    "本地模型部署：可以使用Sentence Transformers库加载本地预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63182b20bfa1d7f2",
   "metadata": {},
   "source": [
    "查询优化：添加纠错和意图识别功能提升用户体验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9c5fd57b8ae7",
   "metadata": {},
   "source": [
    "多模态支持：集成OCR技术处理图片文档（需要额外安装pytesseract）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b4de0fc620b00",
   "metadata": {},
   "source": [
    "知识更新机制：实现自动检测新文档并更新知识库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04d1310e62208f",
   "metadata": {},
   "source": [
    "注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717e82494663f7",
   "metadata": {},
   "source": [
    "该代码使用了开源嵌入模型，如果对性能要求高可以考虑付费模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c827dac8e95b67d",
   "metadata": {},
   "source": [
    "检索结果质量高度依赖于文档内容质量和数量的质量比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f9fa103bf03c4",
   "metadata": {},
   "source": [
    "对于大型文档，建议先进行分块处理（chunking）提高检索效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb112c522ada0d",
   "metadata": {},
   "source": [
    "注意：如果你没有OpenAI API密钥，可以移除对OpenAI的依赖，改用其他开源LLM模型如GPT-J或使用本地语言模型（如HuggingFace上的DistilBERT问答模型）。同时，代码中包含了错误处理和示例文档生成功能，方便你测试系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
