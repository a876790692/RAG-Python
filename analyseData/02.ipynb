{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# main.py\n",
    "import os\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import Analysepdf as Apdf\n",
    "import Analysedocs as Adoc\n",
    "\n",
    "# ===============================\n",
    "# 文件加载函数\n",
    "# ===============================\n",
    "def load_file(file_path, file_type):\n",
    "    \"\"\"根据文件类型加载不同文档\"\"\"\n",
    "    if file_type == 'pdf':\n",
    "        return Apdf.load_pdf(file_path)\n",
    "    elif file_type == 'docx':\n",
    "        return Adoc.load_docx(file_path)\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "# ===============================\n",
    "# 初始化本地 Ollama LLM\n",
    "# ===============================\n",
    "def init_llm(model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"初始化本地 Ollama 模型\"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=model_name,\n",
    "        temperature=0\n",
    "    )\n",
    "# ===============================\n",
    "# RAG 主流程\n",
    "# ===============================\n",
    "def rag_pipeline(documents_dir, user_query, model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"RAG 流程：加载文档 → 向量索引 → 问答\"\"\"\n",
    "    # 1. 加载文档\n",
    "    all_texts = []\n",
    "    for file_name in os.listdir(documents_dir):\n",
    "        file_path = os.path.join(documents_dir, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.txt')):\n",
    "            ext = file_name.split('.')[-1].lower()\n",
    "            text = load_file(file_path, ext)\n",
    "            all_texts.append({'content': text, 'filename': file_name})\n",
    "\n",
    "    if not all_texts:\n",
    "        return \"❌ 没有找到任何文档，请检查目录路径\"\n",
    "\n",
    "    print(\"✅ 文档已加载，创建向量索引...\")\n",
    "\n",
    "    # 2. 嵌入模型（这里用 HuggingFace sentence-transformer，本地可跑）\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 3. 创建向量数据库\n",
    "    metadatas = [{\"source\": doc['filename']} for doc in all_texts]\n",
    "    db = FAISS.from_texts(\n",
    "        [doc['content'] for doc in all_texts],\n",
    "        embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    db.save_local(\"faiss_index\")\n",
    "    print(\"✅ 向量数据库已创建\")\n",
    "\n",
    "    # 4. 初始化本地 LLM\n",
    "    llm = init_llm(model_name=model_name)\n",
    "\n",
    "    # 5. 创建检索器\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # 6. 创建问答链（新版写法）\n",
    "    prompt_template = \"根据以下文档回答问题：\\n{context}\\n\\n问题: {question}\\n答案:\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ 问答链已准备好，正在回答问题...\")\n",
    "\n",
    "    try:\n",
    "        response = qa_chain.invoke({\"query\": user_query})\n",
    "\n",
    "        # 提取答案和来源\n",
    "        answer = response[\"result\"]\n",
    "        source_docs = response.get(\"source_documents\", [])\n",
    "\n",
    "        sources = []\n",
    "        for doc in source_docs:\n",
    "            metadata = doc.metadata\n",
    "            page_content = doc.page_content\n",
    "            if isinstance(metadata, dict) and \"source\" in metadata:\n",
    "                snippet = page_content[:50] + \"...\" if len(page_content) > 50 else page_content\n",
    "                sources.append({\"filename\": metadata[\"source\"], \"text\": snippet})\n",
    "\n",
    "        return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理查询时出错: {e}\")\n",
    "        return \"抱歉，我无法回答这个问题，请提供更多上下文信息。\"\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 主程序入口\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    documents_dir = \"./my_documents\"\n",
    "    query = input(\"请输入你的问题: \")\n",
    "\n",
    "    # 默认使用本地 deepseek-r1:8b 模型\n",
    "    result = rag_pipeline(documents_dir, query, model_name=\"deepseek-r1:8b\")\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        print(\"\\n📌 答案:\")\n",
    "        print(result[\"answer\"])\n",
    "\n",
    "        print(\"\\n📌 相关内容来源（片段）:\")\n",
    "        for src in result[\"sources\"]:\n",
    "            print(f\"- 来源: {src['filename']}\")\n",
    "            print(f\"  内容: {src['text']}\\n\")\n",
    "    else:\n",
    "        print(result)\n"
   ],
   "id": "907e4ea4b9fd43f3"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
