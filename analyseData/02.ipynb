{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# main.py\n",
    "import os\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import Analysepdf as Apdf\n",
    "import Analysedocs as Adoc\n",
    "\n",
    "# ===============================\n",
    "# æ–‡ä»¶åŠ è½½å‡½æ•°\n",
    "# ===============================\n",
    "def load_file(file_path, file_type):\n",
    "    \"\"\"æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½ä¸åŒæ–‡æ¡£\"\"\"\n",
    "    if file_type == 'pdf':\n",
    "        return Apdf.load_pdf(file_path)\n",
    "    elif file_type == 'docx':\n",
    "        return Adoc.load_docx(file_path)\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "# ===============================\n",
    "# åˆå§‹åŒ–æœ¬åœ° Ollama LLM\n",
    "# ===============================\n",
    "def init_llm(model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"åˆå§‹åŒ–æœ¬åœ° Ollama æ¨¡å‹\"\"\"\n",
    "    return OllamaLLM(\n",
    "        model=model_name,\n",
    "        temperature=0\n",
    "    )\n",
    "# ===============================\n",
    "# RAG ä¸»æµç¨‹\n",
    "# ===============================\n",
    "def rag_pipeline(documents_dir, user_query, model_name=\"deepseek-r1:8b\"):\n",
    "    \"\"\"RAG æµç¨‹ï¼šåŠ è½½æ–‡æ¡£ â†’ å‘é‡ç´¢å¼• â†’ é—®ç­”\"\"\"\n",
    "    # 1. åŠ è½½æ–‡æ¡£\n",
    "    all_texts = []\n",
    "    for file_name in os.listdir(documents_dir):\n",
    "        file_path = os.path.join(documents_dir, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.lower().endswith(('.pdf', '.docx', '.txt')):\n",
    "            ext = file_name.split('.')[-1].lower()\n",
    "            text = load_file(file_path, ext)\n",
    "            all_texts.append({'content': text, 'filename': file_name})\n",
    "\n",
    "    if not all_texts:\n",
    "        return \"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ç›®å½•è·¯å¾„\"\n",
    "\n",
    "    print(\"âœ… æ–‡æ¡£å·²åŠ è½½ï¼Œåˆ›å»ºå‘é‡ç´¢å¼•...\")\n",
    "\n",
    "    # 2. åµŒå…¥æ¨¡å‹ï¼ˆè¿™é‡Œç”¨ HuggingFace sentence-transformerï¼Œæœ¬åœ°å¯è·‘ï¼‰\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "    metadatas = [{\"source\": doc['filename']} for doc in all_texts]\n",
    "    db = FAISS.from_texts(\n",
    "        [doc['content'] for doc in all_texts],\n",
    "        embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    db.save_local(\"faiss_index\")\n",
    "    print(\"âœ… å‘é‡æ•°æ®åº“å·²åˆ›å»º\")\n",
    "\n",
    "    # 4. åˆå§‹åŒ–æœ¬åœ° LLM\n",
    "    llm = init_llm(model_name=model_name)\n",
    "\n",
    "    # 5. åˆ›å»ºæ£€ç´¢å™¨\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "    # 6. åˆ›å»ºé—®ç­”é“¾ï¼ˆæ–°ç‰ˆå†™æ³•ï¼‰\n",
    "    prompt_template = \"æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\\n{context}\\n\\né—®é¢˜: {question}\\nç­”æ¡ˆ:\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"âœ… é—®ç­”é“¾å·²å‡†å¤‡å¥½ï¼Œæ­£åœ¨å›ç­”é—®é¢˜...\")\n",
    "\n",
    "    try:\n",
    "        response = qa_chain.invoke({\"query\": user_query})\n",
    "\n",
    "        # æå–ç­”æ¡ˆå’Œæ¥æº\n",
    "        answer = response[\"result\"]\n",
    "        source_docs = response.get(\"source_documents\", [])\n",
    "\n",
    "        sources = []\n",
    "        for doc in source_docs:\n",
    "            metadata = doc.metadata\n",
    "            page_content = doc.page_content\n",
    "            if isinstance(metadata, dict) and \"source\" in metadata:\n",
    "                snippet = page_content[:50] + \"...\" if len(page_content) > 50 else page_content\n",
    "                sources.append({\"filename\": metadata[\"source\"], \"text\": snippet})\n",
    "\n",
    "        return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}\")\n",
    "        return \"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\"\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ä¸»ç¨‹åºå…¥å£\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    documents_dir = \"./my_documents\"\n",
    "    query = input(\"è¯·è¾“å…¥ä½ çš„é—®é¢˜: \")\n",
    "\n",
    "    # é»˜è®¤ä½¿ç”¨æœ¬åœ° deepseek-r1:8b æ¨¡å‹\n",
    "    result = rag_pipeline(documents_dir, query, model_name=\"deepseek-r1:8b\")\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        print(\"\\nğŸ“Œ ç­”æ¡ˆ:\")\n",
    "        print(result[\"answer\"])\n",
    "\n",
    "        print(\"\\nğŸ“Œ ç›¸å…³å†…å®¹æ¥æºï¼ˆç‰‡æ®µï¼‰:\")\n",
    "        for src in result[\"sources\"]:\n",
    "            print(f\"- æ¥æº: {src['filename']}\")\n",
    "            print(f\"  å†…å®¹: {src['text']}\\n\")\n",
    "    else:\n",
    "        print(result)\n"
   ],
   "id": "907e4ea4b9fd43f3"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
